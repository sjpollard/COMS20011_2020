{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.distributions import Normal, Categorical, Bernoulli\n",
    "from torch.distributions import MultivariateNormal as MvNormal\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual\n",
    "mpl.rcParams['figure.max_open_warning'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\Categorical}{{\\rm Categorical}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}[1][]{{\\rm P}_{#1}\\!\\b}\n",
    "\\newcommand{\\Q}[1][]{{\\rm Q}_{#1}\\!\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}[1][]{\\mathbb{E}_{#1}\\!\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\k}{\\mathbf{k}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\La}{\\mathbf{\\Lambda}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sprior}{\\S_\\text{prior}}\n",
    "\\newcommand{\\Spost}{\\S_\\text{post}}\n",
    "\\newcommand{\\mprior}{\\m_\\text{prior}}\n",
    "\\newcommand{\\mpost}{\\m_\\text{post}}\n",
    "\\newcommand{\\Xt}{\\tilde{\\X}}\n",
    "\\newcommand{\\yt}{\\tilde{\\y}}\n",
    "\\newcommand{\\p}{\\mathbf{p}}\n",
    "\\newcommand{\\q}{\\mathbf{q}}\n",
    "\\newcommand{\\l}{\\boldsymbol{\\ell}}\n",
    "\\DeclareMathOperator{\\softmax}{softmax}\n",
    "\\newcommand{\\z}{\\mathbf{z}}\n",
    "\\newcommand{\\norm}{\\bracket{\\lVert}{\\rVert}}\n",
    "\\newcommand{\\Dkl}[2]{D_\\text{KL} \\left( #1 \\middle\\Vert #2 \\right)}\n",
    "$$\n",
    "\n",
    "<h1> Part 5: Unsupervised learning and clustering </h1>\n",
    "\n",
    "<h2> Clustering is not classification... </h2>\n",
    "<h2>...and unsupervised learning is not supervised learning </h2>\n",
    "Clustering is a type of unsupervised learning, which is very, very different from the supervised learning you have seen in my slides so far.\n",
    "\n",
    "In supervised learning, we have a list of input, `x`, output, `y`, pairs as data, and the idea is to learn a function that maps from a new input test point, to a distribution over the corresonding `Y`\n",
    "\n",
    "```\n",
    "#### Supervised learning\n",
    "x : X # Input\n",
    "y : Y # Output\n",
    "    [(X, Y)] -> (X -> Y) \n",
    "```\n",
    "\n",
    "In unsupervised learning/clustering, we only have a list of inputs, and the goal is to compute a (distribution over) a latent variable that somehow summarises the structure of the inputs.\n",
    "\n",
    "```\n",
    "#### Unsupervised learning\n",
    "x : X # Input\n",
    "z : Z # Latent\n",
    "    [X] -> [Z]\n",
    "```\n",
    "\n",
    "<h2> First example </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e580ce6267489d988c6f9029b2f323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = t.cat([\n",
    "    t.randn(50, 2)/5 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/5 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "Z = t.cat([\n",
    "    t.zeros(50).int(),\n",
    "    t.ones(50).int()\n",
    "])\n",
    "\n",
    "#fig = plot.figure()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(7,3), sharey=True)\n",
    "axs[0].set_xlim(-2, 2)\n",
    "axs[1].set_xlim(-2, 2)\n",
    "axs[2].set_xlim(-2, 2)\n",
    "axs[0].set_ylim(-2, 2)\n",
    "axs[0].scatter(X[:, 0], X[:, 1])\n",
    "axs[1].scatter(X[:, 0], X[:, 1], c=Z);\n",
    "axs[2].scatter(X[:, 0], X[:, 1], c=-Z);\n",
    "axs[0].set_title(\"original data, no labels\")\n",
    "axs[1].set_title(\"clustered data\")\n",
    "axs[2].set_title(\"equiv. clustering\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> A classification problem where we need to ignore clusters </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8eb7fa58f1417d913f016653c87536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = Bernoulli(t.sigmoid(100*X[:, 1])).sample()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_xlabel(\"height\")\n",
    "ax.set_ylabel(\"A-level scores\")\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Simplest clustering algorithm: K-means </h2>\n",
    "\n",
    "We start by introducing cluster centers,\n",
    "\\begin{align}\n",
    "  \\m_z &= \\text{cluster center, for cluster indexed $z$}\\\\\n",
    "\\end{align}\n",
    "and integer-valued cluster assignments, $z_i$, describing the cluster associated with the $i$th datapoint,\n",
    "\\begin{align}\n",
    "  z_i &= \\text{integer cluster index for each datapoint, indexed $i$}\\\\\n",
    "\\end{align}\n",
    "\n",
    "The objective is find a bunch of cluster centers such that all datapoints are close to a cluster center.  Formally, we minimise the squared distance between each datapoint and its assigned cluster center,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L(\\z, \\m) &= \\sum_i \\norm{\\x_i - \\m_{z_i}}^2\n",
    "\\end{align}\n",
    "\n",
    "To optimize this algorithm, we use coordinate descent (i.e. we alternate between optimizing $\\z$ and $\\m$).\n",
    "\n",
    "First, we assign each datapoint to the nearest cluster,\n",
    "\n",
    "\\begin{align}\n",
    "  z_i &\\leftarrow \\argmin_{z\\in\\{1...Z\\}} \\norm{\\x_i - \\m_z}^2\n",
    "\\end{align}\n",
    "\n",
    "Then, we put the cluster centers at the mean of the assigned datapoints,\n",
    "\n",
    "\\begin{align}\n",
    "  \\m_z &\\leftarrow \\frac{1}{\\sum_i \\delta_{z, z_i}} \\sum_{i} \\delta_{z, z_i} \\x_i\n",
    "\\end{align}\n",
    "\n",
    "remember that $z$ is an index, whereas $z_i$ is the actual cluster assignment for datapoint $i$.\n",
    "\n",
    "\\begin{align}\n",
    "  \\delta_{z, z_i} = \n",
    "  \\begin{cases}\n",
    "    1 &\\text{if $z=z_i$, i.e. datapoint $i$ is currently assigned to the $z$th cluster}\\\\\n",
    "    0 &\\text{if $z\\neq z_i$, i.e. datapoint $i$ is not in the $z$th cluster}\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "so,\n",
    "\n",
    "\\begin{align}\n",
    "  \\sum_i \\delta_{z, z_i} &= \\text{number of datapoints in cluster $z$}\\\\\n",
    "  \\sum_i \\x_i \\delta_{z, z_i} &= \\text{sum of all the datapoints in cluster $z$}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50151f042a74bc592e7eed37cc366da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# N == number of datapoints\n",
    "# K == number of clusters\n",
    "# D == dimension of the data vectors (usually 2 in our examples)\n",
    "#X.shape  == (N, 1, D) # Data\n",
    "#mu.shape ==    (K, D) # Cluster-centers\n",
    "#q.shape  == (N, K, 1) # One-hot representation of the cluster-assignments for each datapoint\n",
    "#z.shape  == (N,)      # Cluster index for each datapoint\n",
    "\n",
    "t.manual_seed(0)\n",
    "\n",
    "X = t.cat([\n",
    "    t.randn(50, 2)/5 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/5,\n",
    "    t.randn(50, 2)/5 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "def kmeans(X, K):\n",
    "    N, D = X.shape\n",
    "    X = X[:, None, :]\n",
    "    mu = t.randn(K, D)\n",
    "    while True:\n",
    "        sd = ((X - mu)**2).sum(-1) # sd.shape = (N, K)\n",
    "        z = t.argmin(sd, 1)        # z.shape  = (N)\n",
    "        q = t.zeros(N, K, 1)\n",
    "        q[t.arange(N), z, 0] = 1.\n",
    "        print(f\"loss = {loss(X, z, mu).item()}\")\n",
    "        plot(X, z, mu)\n",
    "        yield None\n",
    "        \n",
    "        mu = (q * X).sum(0) / q.sum(0)\n",
    "        print(f\"loss = {loss(X, z, mu).item()}\")\n",
    "        plot(X, z, mu)\n",
    "        yield None\n",
    "        \n",
    "def loss(X, z, mu):\n",
    "    return ((X - mu[z, :][:, None, :])**2).mean()\n",
    "    \n",
    "def plot(X, z, mu):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.scatter(X[:, 0, 0], X[:, 0, 1], c=z);\n",
    "    ax.scatter(mu[:, 0], mu[:, 1], s=100, c='r', label=\"cluster centers\")\n",
    "    ax.legend()\n",
    "    \n",
    "kmeans_iter = iter(kmeans(X, 3))\n",
    "def kmeans_call():\n",
    "    next(kmeans_iter)\n",
    "interact_manual(kmeans_call);\n",
    "\n",
    "#Change marker type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, one strange property of this algorithm is that it assumes \"hard\" cluster assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Soft clustering and the Gaussian Mixture Model (GMM) </h2>\n",
    "\n",
    "The goal of the GMM, is really just to model the density of the data we use the latent variables to give us a better model of the data.\n",
    "\n",
    "For instance, consider fitting a MvNormal to the following data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logPx = -132.896728515625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5542ddfb052b478880f2395851439b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = t.cat([\n",
    "    t.randn(50, 2)/5 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/5 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "N = X.shape[0]\n",
    "mu = X.sum(0)/N\n",
    "cov = ((X - mu).T @ (X-mu))/N\n",
    "mvn = MvNormal(mu, cov)\n",
    "\n",
    "logPx = mvn.log_prob(X).sum()\n",
    "print(f\"logPx = {logPx}\")\n",
    "\n",
    "P = 100\n",
    "x0s = t.linspace(-2, 2, P)[:, None].expand(P, P)\n",
    "x1s = x0s.T\n",
    "xs  = t.stack([x0s.reshape(-1), x1s.reshape(-1)], 1)\n",
    "ps = mvn.log_prob(xs).exp().view(P, P)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "ax.scatter(X[:, 0], X[:, 1]);\n",
    "ax.contour(x0s, x1s, ps);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get a better model for this kind of data?  We use a GMM.\n",
    "\n",
    "The simplest form of a GMM is a density, with $K$ different Gaussians, in different locations, $\\m_k$, and with different covariance matrices, $\\S_k$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x_i} &= \\sum_{z=1}^Z p_z \\N{\\x_i; \\m_z, \\S_z}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this doesn't give us a way to extract any notion of assignment of a datapoint to a mixture component.\n",
    "\n",
    "We therefore write this distribution in terms of a latent variable,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x_i} &= \\sum_{z_i=1}^Z \\P{\\x_i| z_i} \\P{z_i}\n",
    "\\end{align}\n",
    "\n",
    "where, $\\P{z_i}$ is a Categorical (i.e. a distribution over integers from $1$ to $Z$), and represents the mixture component for the $i$th data point, and $\\P{x_i| z_i}$ is a _single_ Gaussian, (corresponding to the $z_i$th mixture component),\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{z_i} &= \\Categorical{z_i; \\p} = p_{z_i}\\\\\n",
    "  \\P{\\x_i| z_i} &= \\N{\\x_i; \\m_{z_i}, \\S_{z_i}}.\n",
    "\\end{align}\n",
    "\n",
    "The substituting in the values of $\\P{z_i}$ and $\\P{x_i| z_i}$, we get the same expression as above, except use $z_i$ as the summation index, instead of $z$\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x_i} &= \\sum_{z_i=1}^Z \\P{\\x_i| z_i} \\P{z_i} = \\sum_{z_i=1}^Z p_{z_i} \\N{\\x_i; \\m_{z_i}, \\S_{z_i}}.\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use Bayes theorem to give a posterior distribution over $z_i$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{z_i| \\x_i} &\\propto \\P{\\x_i| z_i} \\P{z_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggests an algorithm similar to K-means: first compute the posterior over $z_i$ (E-step), then update the parameters of the Gaussians, $\\m_z$, using a mean weighted by the posterior (M-step).\n",
    "\n",
    "In particular, the **E-step** updates the posterior over $z_i$ for a given $x_i$, and \"saves\" the posterior into $\\q$.  The denominator normalizes the distribution so that it sums to 1.\n",
    "\\begin{align}\n",
    "  q_{i; z} &\\leftarrow \\P{z_i=z| x_i} = \\frac{\\P{\\x_i| z_i=z} \\P{z_i=z}}{\\sum_{z'} \\P{\\x_i| z_i=z'} \\P{z_i=z'}}\n",
    "\\end{align}\n",
    "and the **M-step**, is a weighted average of $\\x_i$'s, with the weights corresponding to the degree to which that datapoint is assigned to that cluster,\n",
    "\\begin{align}\n",
    "  \\m_z &\\leftarrow \\frac{\\sum_i \\x_i q_{i, z}}{\\sum_i q_{i, z}}\n",
    "\\end{align}\n",
    "\n",
    "In the case where we are very sure about the cluster assignments, the posteriors become equal to the hard-assignment case, because $\\P{z_i=z| x_i}$ is $1$ for the \"right\" assignment and $0$ otherwise,\n",
    "\n",
    "\\begin{align}\n",
    "  q_{i; z} &= \\P{z_i=z| x_i} = \\delta_{z_i, z}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54a87e0576f4b15ac733d0b3f553bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.manual_seed(0)\n",
    "\n",
    "X = t.cat([\n",
    "    t.randn(50, 2)/3 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/3 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "def gmm_em(X, K):\n",
    "    N, D = X.shape\n",
    "    X = X[:, None, :]\n",
    "    mu  = t.randn(K, D)\n",
    "    cov = 0.5*t.eye(D, D).expand(K, -1, -1)\n",
    "    while True:\n",
    "        # unnormalized posterior probability\n",
    "        q = MvNormal(mu, cov).log_prob(X).exp()\n",
    "        # normalized posterior probability\n",
    "        q = q / q.sum(1, keepdim=True)\n",
    "        # expand out last such that q.shape = (N, K, 1)\n",
    "        q = q[:, :,  None]\n",
    "        plot_gmm(X, q, mu, cov)\n",
    "        yield None\n",
    "        \n",
    "        #weighted mean\n",
    "        mu = (q * X).sum(0) / q.sum(0)\n",
    "        plot_gmm(X, q, mu, cov)\n",
    "        yield None\n",
    "\n",
    "        \n",
    "def plot_gmm(X, q, mu, cov):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.scatter(X[:, 0, 0], X[:, 0, 1], c=q[:, 0, 0]);\n",
    "    ax.scatter(mu[:, 0], mu[:, 1], s=100, c='r', label=\"cluster centers\")\n",
    "    ax.legend()\n",
    "\n",
    "    \n",
    "gmm_em_iter = iter(gmm_em(X, 2))\n",
    "def gmm_em_call():\n",
    "    next(gmm_em_iter)\n",
    "interact_manual(gmm_em_call);\n",
    "\n",
    "#Change marker type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> What is the objective function for EM? [Non-examinable] </h2>\n",
    "\n",
    "Turns out that this is a very subtle question.\n",
    "\n",
    "We need to define an \"approximate posterior\", corresponding to the current cluster assignments,\n",
    "\n",
    "\\begin{align}\n",
    "  \\Q[\\q]{z_i} &= \\Categorical{z_i; \\q_{i}} = q_{i, z_i}\n",
    "\\end{align}\n",
    "\n",
    "We can write down the \"evidence lower-bound objective\" (ELBO).  Note that the \"model evidence\" is $\\log \\P{\\x}$, and the ELBO can be defined as,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P[\\m]{\\x} \\geq \\L\\b{\\m, \\q} &= \\log \\P[\\m]{\\x}- \\Dkl{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}}\n",
    "\\end{align}\n",
    "\n",
    "Where the ELBO is a lower-bound because the KL-divergence is non-negative,\n",
    "\n",
    "\\begin{align}\n",
    "  0 \\leq \\Dkl{\\Q[q]{\\z}}{\\P[\\m]{\\z| \\x}} = \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}}}\n",
    "\\end{align}\n",
    "\n",
    "And the KL-divergence equals zero when the approximate posterior equals the true posterior, $\\Q{\\z} = \\P{\\z| \\x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Expectation (E) step </h4>\n",
    "\n",
    "Thus, the E-step consists of maximizing the ELBO wrt to the parameters of the approximate posterior, $\\q$.\n",
    "\n",
    "\\begin{align}\n",
    "  \\q \\leftarrow \\argmax_{\\q} \\L\\b{\\m, \\q}\n",
    "\\end{align}\n",
    "\n",
    "as $\\log \\P[\\m]{\\x}$ does not depend on $\\q$ this is equivalent to,\n",
    "\n",
    "\\begin{align}\n",
    "  \\q \\leftarrow \\argmax_{\\q} \\Dkl{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}} \n",
    "\\end{align}\n",
    "\n",
    "This KL-divergence is minimized by setting $\\q$ to the true posterior (i.e. the E-step that we saw above), concretely, we can achieve this by setting,\n",
    "\n",
    "\\begin{align}\n",
    "  q_{i, z} \\leftarrow \\P{z_i=z| \\x_i}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Maximization (M) step </h3>\n",
    "\n",
    "The goal is,\n",
    "\\begin{align}\n",
    "  \\m \\rightarrow \\argmax_{\\m} \\L\\b{\\m, \\q}\n",
    "\\end{align}\n",
    "\n",
    "Both terms in the previous form for the ELBO depend on $\\m$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L\\b{\\m, \\q} &= \\log \\P[\\m]{\\x} + \\Dkl{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}}\n",
    "\\end{align}\n",
    "\n",
    "so we need to rearrange to get a good update for $\\m$.\n",
    "\n",
    "In particular,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L\\b{\\m, \\q} &= \\log \\P[\\m]{\\x} - \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\log \\P[\\m]{\\x} + \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\P[\\m]{\\z| \\x}}{\\Q[\\q]{\\z}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\E[{\\Q[\\q]{\\z}}]{\\log \\P[\\m]{\\x} + \\log \\frac{\\P[\\m]{\\z| \\x}}{\\Q[\\q]{\\z}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\P[\\m]{\\z| \\x} \\P[\\m]{\\x}}{\\Q[\\q]{\\z}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\P[\\m]{\\x, \\z}}{\\Q[\\q]{\\z}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\E[{\\Q[\\q]{\\z}}]{\\log \\P[\\m]{\\x, \\z}} - \\E[{\\Q[\\q]{\\z}}]{\\Q[\\q]{\\z}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that the second term doesn't depend on $\\m$, so,\n",
    "\n",
    "\\begin{align}\n",
    "  \\m \\rightarrow \\argmax_{\\m} \\E[{\\Q[\\q]{\\z}}]{\\log \\P[\\m]{\\x, \\z}}\n",
    "\\end{align}\n",
    "\n",
    "And this is just maximum-likelihood fitting of the parameters of the mixtures (here, the mixture-means), with datapoints weighted by $\\Q{\\z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
